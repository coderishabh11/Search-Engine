{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ec1284",
   "metadata": {},
   "source": [
    "# Searching using Qdrant Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e86fdb",
   "metadata": {},
   "source": [
    "## Lets Begin by first downloading the required libraries for this notebook.\n",
    "\n",
    "\n",
    "*   **qdrant-client** is used for communicating with the qdrant server\n",
    "*   **datasets** is used for downloading the dataset\n",
    "*   **tqdm** is used for the progress bars\n",
    "*   **sentence-transformers**  is used for generating and working with sentence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9778925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qdrant-client in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.14.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.26.14 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qdrant-client) (1.26.16)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qdrant-client) (2.7.0)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qdrant-client) (1.24.3)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qdrant-client) (1.56.2)\n",
      "Requirement already satisfied: pydantic<2.0,>=1.8 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qdrant-client) (1.10.12)\n",
      "Requirement already satisfied: httpx[http2]>=0.14.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qdrant-client) (0.24.1)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=4.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qdrant-client) (4.5.0)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qdrant-client) (1.56.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: xxhash in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: scipy in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.31.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: protobuf<5.0dev,>=4.21.6 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client) (4.21.12)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client) (58.1.0)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx[http2]>=0.14.0->qdrant-client) (0.17.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx[http2]>=0.14.0->qdrant-client) (2023.7.22)\n",
      "Requirement already satisfied: idna in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx[http2]>=0.14.0->qdrant-client) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx[http2]>=0.14.0->qdrant-client) (1.3.0)\n",
      "Requirement already satisfied: h2<5,>=3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx[http2]>=0.14.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client) (4.0.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client) (6.0.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore<0.18.0,>=0.15.0->httpx[http2]>=0.14.0->qdrant-client) (0.14.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore<0.18.0,>=0.15.0->httpx[http2]>=0.14.0->qdrant-client) (3.7.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx[http2]>=0.14.0->qdrant-client) (1.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant-client) (306)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->sentence-transformers) (1.3.1)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->sentence-transformers) (8.1.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision->sentence-transformers) (10.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install qdrant-client datasets tqdm sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50131ad",
   "metadata": {},
   "source": [
    "With the required packages installed we can get started. Lets begin by launching the Qdrant service. The file being run is the docker-compose.yaml found in the folder of this file. This command launches a Qdrant standalone instance which we will use for this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "649c01c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=\"2023-08-02T13:46:01+05:30\" level=warning msg=\"a network with name milvus exists but was not created for project \\\"qdrant\\\".\\nSet `external: true` to use an existing network\"\n",
      " Container milvus-etcd  Creating\n",
      " Container milvus-minio  Creating\n",
      "Error response from daemon: Conflict. The container name \"/milvus-minio\" is already in use by container \"b1747c11006c2da61c7feab994abddf076a2bf2df4ad31bd1d67fdd3a06ccdd8\". You have to remove (or rename) that container to be able to reuse that name.\n"
     ]
    }
   ],
   "source": [
    "! docker compose up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5faabfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.notebook import tqdm\n",
    "from qdrant_client import QdrantClient, models\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "391dd497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Qdrant Client\n",
    "qdrant_client = QdrantClient(':memory:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea410d58",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "  With Qdrant up and running, we can start gathering our data. Hugging Face Datasets provides a wide range of user datasets, and in this scenario, we will utilize the \"movie-metadata\" dataset from HuggingLearners. This dataset comprises metadata pairs for more than 8,000 movies. Our objective is to generate embeddings for each movie description and store them in Qdrant, alongside the movie's title, genre, release year, and rating.\n",
    "  \n",
    "The raw parsed data can be accessed via this https://huggingface.co/datasets/hugginglearners/netflix-shows/tree/main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d2b61e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from CSV\n",
    "csv_path = '../netflix_titles.csv'\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75cc96c",
   "metadata": {},
   "source": [
    "## Preprocess the Data\n",
    "  In this step, we will preprocess data which is required for analysis and modeling. It involves tasks like selecting relevant columns, handling missing values, removing duplicates, scaling features, and encoding categorical variables. It ensures data quality, consistency, and suitability for further processing, improving analysis and modeling accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cf53ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # Drop duplicates, if any\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Handling missing values\n",
    "    # For numeric columns, fill missing values with median\n",
    "    numeric_cols = df.select_dtypes(include='number').columns\n",
    "    for col in numeric_cols:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "    # For categorical columns, fill missing values with most frequent value\n",
    "    categorical_cols = df.select_dtypes(include='object').columns\n",
    "    for col in categorical_cols:\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "    # Convert all text data to lowercase\n",
    "    df = df.applymap(lambda x: x.lower() if isinstance(x, str) else '')\n",
    "\n",
    "    # Prepare NLTK for english Language Preprocessing\n",
    "    eng_stopwords = set(stopwords.words('english'))  # Set of English stopwords\n",
    "    stemmer = SnowballStemmer('english')  # Snowball Stemmer for English language\n",
    "\n",
    "    # Preprocess text in categorical columns using NLTK\n",
    "    for col in categorical_cols:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x) if word.lower() not in eng_stopwords]))\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "df_movies = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c45706ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(input_string):\n",
    "    # Convert the input string to lowercase\n",
    "    input_string = input_string.lower()\n",
    "\n",
    "    # Prepare NLTK for English Language Preprocessing\n",
    "    eng_stopwords = set(stopwords.words('english'))  # Set of English stopwords\n",
    "    stemmer = SnowballStemmer('english')  # Snowball Stemmer for Englsih language\n",
    "\n",
    "    # Tokenize the input string\n",
    "    words = word_tokenize(input_string)\n",
    "\n",
    "    # Remove English stopwords and apply stemming\n",
    "    preprocessed_words = [stemmer.stem(word) for word in words if word.lower() not in eng_stopwords]\n",
    "\n",
    "    # Combine the preprocessed words into a single string\n",
    "    preprocessed_string = ' '.join(preprocessed_words)\n",
    "\n",
    "    return preprocessed_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed3d52a",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "  The below code snippet encodes movie descriptions using SentenceTransformer, a pre-trained model for sentence embeddings. It iterates over the movie descriptions in the DataFrame (df_movie) in batches and uses the **multi-qa-MiniLM-L6-cos-v1model** to encode the descriptions into dense vectors. The encoded vectors are then saved as a NumPy array in the 'data/vectors_movies.npy' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2ee9a7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca53fd40c83e42f6b0e4dc4d9638926a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encode movie descriptions using SentenceTransformer\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1', device='cpu')\n",
    "vectors = []\n",
    "batch_size = 64\n",
    "batch = []\n",
    "for row in tqdm(df_movies.itertuples()):\n",
    "    description = row.description\n",
    "    batch.append(description)\n",
    "    if len(batch) >= batch_size:\n",
    "        vectors.append(model.encode(batch))\n",
    "        batch = []\n",
    "\n",
    "if len(batch) > 0:\n",
    "    vectors.append(model.encode(batch))\n",
    "    batch = []\n",
    "\n",
    "vectors = np.concatenate(vectors)\n",
    "np.save('data/vectors_movies.npy', vectors, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ed2e08",
   "metadata": {},
   "source": [
    "## Qdrant Setup\n",
    "\n",
    "*   **CODE_DIR** = os.getcwd(): Assigns the current working directory to CODE_DIR\n",
    "*   **ROOT_DIR** = os.path.dirname(CODE_DIR): Assigns the parent directory of CODE_DIR to ROOT_DIR\n",
    "*   **DATA_DIR** = os.path.join(CODE_DIR, 'data'): Creates a path for data-related files by joining CODE_DIR and the subdirectory name 'data', assigned to DATA_DIR\n",
    "*   **COLLECTION_NAME** = 'movies': Sets the collection name in Qdrant as 'movies\n",
    "*   **QDRANT_HOST** = os.environ.get('QDRANT_HOST', 'localhost'): Retrieves the QDRANT_HOST value from environment variables, defaulting to 'localhost' if not set.\n",
    "*   **QDRANT_PORT** = os.environ.get('QDRANT_PORT', 6333): Retrieves the QDRANT_PORT value from environment variables, defaulting to 6333 if not set.\n",
    "*   **vectors_path** = os.path.join(DATA_DIR, 'vectors_movies.npy'): Creates a path for the encoded vectors file by joining DATA_DIR and the filename 'vectors_movies.npy', assigned to vectors_path.\n",
    "*   **vectors** = np.load(vectors_path): Loads encoded vectors from vectors_path using np.load(), assigned to vectors\n",
    "*   **vector_size** = vectors.shape[1]: Retrieves the size of the vectors by accessing the shape of vectors and taking the second element ([1] index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0ebe7b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_DIR = os.getcwd()\n",
    "ROOT_DIR = os.path.dirname(CODE_DIR)\n",
    "DATA_DIR = os.path.join(CODE_DIR, 'data')\n",
    "COLLECTION_NAME = 'movies'\n",
    "QDRANT_HOST = os.environ.get('QDRANT_HOST', 'localhost')\n",
    "QDRANT_PORT = os.environ.get('QDRANT_PORT', 6333)\n",
    "vectors_path = os.path.join(DATA_DIR, 'vectors_movies.npy')\n",
    "vectors = np.load(vectors_path)\n",
    "vector_size = vectors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "584dc210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Qdrant client and collection creation\n",
    "qdrant_client = QdrantClient(':memory:')  # QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)\n",
    "qdrant_client.recreate_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=models.VectorParams(size=vector_size, distance='Cosine')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "797b644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Upload vectors and payload data to Qdrant\n",
    "BATCH_SIZE = 64\n",
    "qdrant_client.upload_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors=vectors,\n",
    "    payload=df.to_dict(orient='records'),\n",
    "    ids=None,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    parallel=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e9988a",
   "metadata": {},
   "source": [
    "## Make a Search function\n",
    "  Now that all the preparations are complete, let's build the search class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c110d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_movies(qdrant_client, collection_name, query, query_filter=None, top_k=3):\n",
    "    query_text = preprocess_query(query)\n",
    "    query_vector = model.encode(query_text).tolist()\n",
    "    hits = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        query_filter=query_filter,\n",
    "        limit=top_k\n",
    "    )\n",
    "    print('Search Results:')\n",
    "    for i, hit in enumerate(hits):\n",
    "        print(f'\\nResult {i + 1}:')\n",
    "        print('Title:', hit.payload.get('title', 'N/A'))\n",
    "        print('Type:', hit.payload.get('type', 'N/A'))\n",
    "        print('Release Year:', hit.payload.get('release_year', 'N/A'))\n",
    "        print('Rating:', hit.payload.get('rating', 'N/A'))\n",
    "        print('Description:', hit.payload.get('description', 'N/A'))\n",
    "        print('Score:', hit.score)\n",
    "        print('-'*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6c55113b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results:\n",
      "\n",
      "Result 1:\n",
      "Title: Pop Team Epic\n",
      "Type: TV Show\n",
      "Release Year: 2018\n",
      "Rating: TV-14\n",
      "Description: This animated adaptation of the quirky four-panel comic brings the random exploits of Popuko and Pipimi to life.\n",
      "Score: 0.47200582458503304\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Result 2:\n",
      "Title: Gabriel lglesias: I’m Sorry For What I Said When I Was Hungry\n",
      "Type: Movie\n",
      "Release Year: 2016\n",
      "Rating: TV-14\n",
      "Description: Hawaiian-shirt enthusiast Gabriel \"Fluffy\" Iglesias finds the laughs in racist gift baskets, Prius-driving cops and all-female taco trucks.\n",
      "Score: 0.4275715285325412\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Result 3:\n",
      "Title: Enter the Anime\n",
      "Type: Movie\n",
      "Release Year: 2019\n",
      "Rating: TV-MA\n",
      "Description: What is anime? Through deep-dives with notable masterminds of this electrifying genre, this fast-paced peek behind the curtain seeks to find the answers.\n",
      "Score: 0.4251402543698447\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "search_movies(qdrant_client, COLLECTION_NAME, 'fluffy animal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e53be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
